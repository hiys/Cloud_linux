1 昨天已经搭建好 elasticsearch 集群
清除所有 index
导入 logs.jsonl 日志

mapping：
映射：创建索引的时候，可以预先定义字段的类型及相关属性。
作用：这样会让索引建立得更加的细致和完善。
分类：静态映射和动态映射。
动态映射：自动根据数据进行相应的映射。
静态映射：自定义字段映射数据类型。

kibana部分

1.kibana的概念及特点。
概念：数据可视化平台工具
特点：
    - 灵活的分析和可视化平台
    - 实时总结和流数据的图表
    - 为不同的用户显示直观的界面
    - 即时分享和嵌入的仪表板

2.kibana的安装配置。
rpm -ivh kibana-4.5.2-1.x86_64.rpm

#配置 kibana 
/opt/kibana/config/kibana.yml
server.port: 5601
server.host: "0.0.0.0"
elasticsearch.url: "http://192.168.4.13:9200"
kibana.index: ".kibana"
kibana.defaultAppId: "discover"
elasticsearch.pingTimeout: 1500
elasticsearch.requestTimeout: 30000
elasticsearch.startupTimeout: 5000

通过图形页面展示，注意时间是 2015 年，需要调整时间才能正常显示

logstash部分
Logstash 是 Elastic Stack 的中央数据流引擎，
用于收集、丰富和统一所有数据，而不管格式或模式。
当与Elasticsearch，Kibana，及 Beats 共同使用
的时候便会拥有特别强大的实时处理能力

 logstash是一个数据分析软件，主要目的是分析log日志。
整一套软件可以当作一个MVC模型，
logstash是controller层，
Elasticsearch是一个model层，
kibana是view层。
  首先将数据传给logstash，
它将数据进行过滤和格式化（转成JSON格式），
然后传给Elasticsearch进行存储、建搜索的索引，
kibana提供前端的页面再进行搜索和图表可视化，
它是调用Elasticsearch的接口返回的数据进行可视化。
logstash和Elasticsearch是用Java写的，
kibana使用node.js框架。

这个软件官网有很详细的使用说明，https://www.elastic.co/，除了docs之外，
还有视频教程

3.logstash的概念及特点。
概念：logstash是一个数据采集、加工处理以及传输(输出)的工具。
特点：
    - 所有类型的数据集中处理
    - 不同模式和格式数据的正常化
    - 自定义日志格式的迅速扩展
    - 为自定义数据源轻松添加插件

https://www.elastic.co/cn/videos

Beats 是一系列轻量级的数据收集器，
直接运行在终端设备并向 Elasticsearch 发送数据信息。
Packetbeat 用来收集网络数据。

安装Logstash
rpm -ivh logstash-2.3.4-1.noarch.rpm
Logstash 依赖 java 环境，需要安装 java-1.8.0-openjdk

安装之后，创建第一个测试的配置文件
/etc/Logstash/logstash.conf
input{ stdin{} } 
filter{  }
output{ stdout{} }

使用 logstash -f logstash.conf 启动，如果输入数据能看到返回证明 logstash 安装正确

logstash 数据处理结构
                          | -------------------------logstash---------------------------|
  ｛数据源｝ -->{ input{数据接收} -- filter{数据处理} -- output{数据发送}  } --> {ES集群}
                          |--------------------------logstash---------------------------|


布尔值类型:  ssl_enable => true
字节类型:     bytes => "1MiB"
字符串类型:  name => "xkops"
数值类型:     port => 22
数组:            match => ["datetime","UNIX"]
哈希:            options => {key1 => "value1",key2 => "value2"}
编码解码:     codec => "json"
路径:            file_path => "/tmp/filename"
注释:       #

条件判断：
等于:       ==
不等于:     !=
小于:       <
大于:       >
小于等于:   <=
大于等于:   >=
匹配正则:   =~
不匹配正则: !~
包含:        in
不包含:     not in 
与:	and
或:	or
非与:          nand
非或:	xor
复合表达式: ()
取反符合:   !()

logstash-file 插件: 从文件读取，在屏幕输出
file插件字段解释：
codec =>  #可选项，默认是plain，可设置其他编码方式。
discover_interval => #可选项，logstash多久检查一下path下有新文件，默认15s。
exclude => #可选项，排除path下不想监听的文件。
sincedb_path => #可选项，记录文件以及文件读取信息位置的数据文件。~/.sincedb_xxxx
sincedb_write_interval => #可选项，logstash多久写一次sincedb文件，默认15s.
stat_interval => #可选项，logstash多久检查一次被监听文件的变化，默认1s。
start_position => #可选项，logstash从哪个位置读取文件数据，默认从尾部，值为：end。初次导入，设置为：beginning。
path => #必选项，配置文件路径，可定义多个。
tags => #可选项，在数据处理过程中，由具体的插件来添加或者删除的标记。
type => #可选项，自定义处理时间类型。比如nginxlog。

input{
    file{
        start_position => "beginning"
        sincedb_path => "/var/lib/logstash/sincedb-access"
        path => ["/tmp/blog","/tmp/alog"]
        type => 'filelog'
    }
}

filter{  }
output{ stdout{} }

logstash-tcp 插件：从网络读取，在屏幕输出
tcp插件字段解释：
add_field => #可选项，默认{}。
codec => #可选项，默认plain。
data_timeout => #可选项，默认-1。
host => #可选项，默认0.0.0.0。
mode => #可选项，值为["server","client"]之一，默认为server。
port => #必选，端口。
ssl_cacert => #可选项，定义相关路径。
ssl_cert => #可选项，定义相关路径。
ssl_enable => #可选项，默认为false。
ssl_key => #可选项，定义相关路径。
ssl_key_passphrase => #可选项，默认nil
ssl_verify => #可选项，默认false。
tags => #可选项
type => #可选项
input{
    tcp{
        host => "0.0.0.0"
        port => 8888
        type => "tcplog"
    }
}
filter{  }
output{ stdout{} }

在服务器上启动 logstash , 在客户机上使用 shell 脚本测试
function tcpmsg() {
    exec 9<>/dev/tcp/192.168.4.10/8888
    echo -ne "${1}\r\n" >&9
    exec 9<&-
}

logstash-udp插件：
udp插件字段解释：
add_field => #可选项，默认{}。
host => #可选项，默认0.0.0.0。
queue_size => #默认2000
tags => #可选项
type => #可选项
workers => #默认为2

input{
    udp{
        host => "192.168.4.10"
        port => 9999
    }
}
filter{  }
output{ stdout{} }

在服务器上启动 logstash , 在客户机上使用 shell 脚本测试
function udpmsg() {
    exec 9<>/dev/udp/192.168.4.10/9999
    echo -ne "${1}\r" >&9
    exec 9<&-
}

logstash-syslog 插件：

input{
    syslog{
        host => "192.168.4.10"
        port => 514
        type => "syslog"
    }
}
filter{  }
output{ stdout{} }

在服务器启动 logstash，在客户机修改 /etc/rsyslog.conf 配置问件，添加
*.*	@@192.168.4.10:514
重新启动 rsyslog 服务
systemctl restart rsyslog
使用明令写入 syslog 进行测试
logger -p local0.info -t test_logstash 'test message'
输入命令以后可以在 /var/log/messages 看到，在 logstash 服务器端也同时能看到输出

codec类插件
常用的插件：plain、json、json_lines、rubydebug、multiline等

input{
    file{
        start_position => "beginning"
        sincedb_path => "/dev/null"
        path => ["/tmp/alog"]
        type => 'filelog'
        codec => "json"
    }
}
output{
    stdout{ codec => "rubydebug" }
}

利用 rubydebug 方便调试，如果输入在文件是 json 在 input 指定编码格式

filter grok插件：解析各种非结构化的日志数据插件
grok有丰富的patterns,查看方式
/opt/logstash/vendor/bundle/jruby/1.9/gems/logstash-patterns-core-2.0.5/patterns/grok-patterns

filter{
    grok{
        match => ["message","%{IP:ip} %{WORD:method} %{URIPATH:uri} %{NUMBER:bytes} %{NUMBER:duration}"]
    }
}

grok 使用正则表达式把飞结构化的数据结构化在分组匹配，正则表达式需要根据具体数据结构编写，虽然编写困难，但适用性极广，几乎可以应用于各类数据

最后是一个完整的 Logstash 的配置文件，使用Logstash 收集数据，格式化以后放入 ES 集群
input{
    file{
        start_position => "beginning"
        sincedb_path => "/dev/null"
        path => ["/tmp/alog"]
        type => 'filelog'
        codec => "json"
    }
}

filter{
}
output{ 
    if [type] == "filelog"{
    elasticsearch {
        hosts => ["192.168.4.15:9200"]
        index => "weblog"
        flush_size => 2000
        idle_flush_time => 10
    }}
}

放入集群以后的数据可以通过 kibana 展示
在生产环境中，我们往往还需要配置 redis 用来缓存 或 filebeat 用来收集日志，这里给出简单的配置样例，需要更深入学习的同学请查看官方文档

https://github.com/logstash-plugins

redis 配置
input{
    redis{
    host => 'redis-server'
    port => '6379'
    data_type => 'list'
    key => 'lb'
    codec => 'json'
    }
}

filebeat 配置
input {
    beats {
        port => 5044
        codec => "json"
    }
}

filebeat 客户端相关配置文件
filebeat:
  prospectors:
    -
      paths:
        - /root/logs.jsonl
      document_type: logstash
    - 
      paths:
        - /root/shakespeare.json
      document_type: shakespeare
    - 
      paths:
        - /root/accounts.json
      document_type: account
    
  registry_file: /var/lib/filebeat/registry
output:
  logstash:
    hosts: ["192.168.4.10:5044"]
shipper:
logging:
  files:


logstash 插件 input 详解

标准输入 stdin{}
input{
    stdin{
        add_field => {"key" => "value"} #向事件添加一个字段
        codec => "plain" #默认是line, 可通过这个参数设置编码方式
        tags => ["std"] #添加标记
        type => "std" #添加类型
        id => 1 #添加一个唯一的ID, 如果没有指定ID, 那么将生成一个ID
        enable_metric => true #是否开启记录日志, 默认true
    }
}

# stdin官方参考: https://www.elastic.co/guide/en/logstash/current/plugins-inputs-stdin.html

文件输入 file{}
input{
    file{
        path => ["/var/log/nginx/access.log", "/var/log/nginx/error.log"] #处理的文件的路径, 可以定义多个路径
        exclude => "*.zip" #匹配排除
        sincedb_path => "/data/" #sincedb数据文件的路径, 默认<path.data>/plugins/inputs/file
        codec => "plain" #默认是plain,可通过这个参数设置编码方式
        tags => ["nginx"] #添加标记
        type => "nginx" #添加类型
        discover_interval => 2 #每隔多久去查一次文件, 默认15s
        stat_interval => 1 #每隔多久去查一次文件是否被修改过, 默认1s
        start_position => "beginning" #从什么位置开始读取文件数据, beginning和end, 默认是结束位置end
    }
}

# file官方参考: https://www.elastic.co/guide/en/logstash/current/plugins-inputs-file.html

TCP/UDP输入 tcp/udp{}

input{
    tcp{
       port => 8888 #端口
       mode => "server" #操作模式, server:监听客户端连接, client:连接到服务器
       host => "0.0.0.0" #当mode为server, 默认0.0.0.0 , 指定监听地址; 
                                 当mode为client, 指定连接地址, 例如host => "www.baidu.com"

       ssl_enable => false #是否启用SSL, 默认false
       ssl_cert => "" #SSL证书路径
       ssl_extra_chain_certs => [] #将额外的X509证书添加到证书链中
       ssl_key => "" #SSL密钥路径
       ssl_key_passphrase => "nil" #SSL密钥密码, 默认nil
       ssl_verify => true #核实与CA的SSL连接的另一端的身份
       tcp_keep_alive => false #TCP是否保持alives
    }
}
input{
    udp{
       buffer_size => 65536 #从网络读取的最大数据包大小, 默认65536
       host => 0.0.0.0 #监听地址
       port => 8888 #端口
       queue_size => 2000 #在内存中保存未处理的UDP数据包的数量, 默认2000
       workers => 2 #处理信息包的数量, 默认2
    }
}

UDP协议

    UDP是无连接通信协议，
即在数据传输时，数据的发送端和接收端不建立逻辑连接。
简单来说，当一台计算机向另外一台计算机发送数据时，
发送端不会确认接收端是否存在，就会发出数据，
同样接收端在收到数据时，也不会向发送端反馈是否收到数据。

    由于使用UDP协议消耗资源小，通信效率高，
所以通常都会用于音频、视频和普通数据的传输例如视频会议都使用UDP协议，
因为这种情况即使偶尔丢失一两个数据包，也不会对接收结果产生太大影响。

    但是在使用UDP协议传送数据时，由于UDP的面向无连接性，不能保证数据的完整性，
因此在传输重要数据时不建议使用UDP协议。


# tcp官方参考: https://www.elastic.co/guide/en/logstash/current/plugins-inputs-tcp.html
# udp官方参考: https://www.elastic.co/guide/en/logstash/current/plugins-inputs-udp.html

syslog输入 syslog{}

input{
    syslog{
       host => 0.0.0.0 #监听地址, 默认0.0.0.0
       port => "8888" #端口
    }
}

# syslog官方参考: https://www.elastic.co/guide/en/logstash/current/plugins-inputs-syslog.html



