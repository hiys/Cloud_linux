hadoop 安装 （单机模式）

禁用 selinux 和 iptables
禁用 selinux 和 iptables
禁用 selinux 和 iptables
配置 /etc/hosts 保证所有主机域名能够相互解析
配置 /etc/hosts 保证所有主机域名能够相互解析
配置 /etc/hosts 保证所有主机域名能够相互解析

1、安装 java 
yum install java-1.8.0-openjdk -y

验证：
java -version

2、安装 jps
yum install java-1.8.0-openjdk-devel -y

验证：
jps

3、安装 hadoop 
tar zxf hadoop-2.7.3.tar.gz
mv hadoop-2.7.3 /usr/local/hadoop

修改配置文件的运行环境：
/usr/local/hadoop/etc/hadoop/hadoop-env.sh
export JAVA_HOME="/usr/lib/jvm/java-1.8.0-openjdk-1.8.0.65-3.b17.el7.x86_64/jre"
export HADOOP_CONF_DIR="/usr/local/hadoop/etc/hadoop"

验证：
cd /usr/local/hadoop
./bin/hadoop version

统计分析热词
创建数据源
mkdir input
在这个文件夹里面放入需要统计分析的数据
cp *.txt input/

统计分析1  单词出现的频率
./bin/hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.3.jar wordcount input output

统计分析2  某一个关键词出现的频率，例如 dfs 这个词前面字母是 h 的出现的频率
./bin/hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.3.jar grep input output1 '(?<=h)dfs'

排错 1
提示  JAVA_HOME is not set and could not be found
表示  JAVA_HOME 没有设置
解决方法：
设置 hadoop-env.sh 里面的 JAVA_HOME 或在运行脚本前面加入前置变量设置
JAVA_HOME="/usr/lib/jvm/java-1.8.0-openjdk-1.8.0.65-3.b17.el7.x86_64/jre" ./bin/hadoop jar ./share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.3.jar wordcount input output

排错 2
提示 java.net.UnknownHostException: host: host: unknown error
	at java.net.InetAddress.getLocalHost(InetAddress.java:1505)
表示主机名没有 IP 解析
解决方法：
在 /etc/hosts 里面增加 主机名 IP 对应关系

排错 3
提示：17/07/24 23:10:46 INFO jvm.JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized
org.apache.hadoop.mapred.FileAlreadyExistsException: Output directory file:/usr/local/hadoop/output already exists
表示输出的文件目录已经存在
解决方法：
删除已经存在的目录或更改结果保存位置

伪分布式配置：

xml 配置格式
<property>
        <name>关键字</name>
        <value>变量值</value>
        <description> 描述 </description>
</property>

配置文件路径 /usr/local/hadoop/etc/hadoop/
1 配置 hadoop-env.sh
export JAVA_HOME="/usr/lib/jvm/java-1.8.0-openjdk-1.8.0.65-3.b17.el7.x86_64/jre"
export HADOOP_CONF_DIR="/usr/local/hadoop/etc/hadoop"
查找 JAVA_HOME
readlink -f $(which java)

2 配置 core-site.xml
https://hadoop.apache.org/docs/r2.7.3/hadoop-project-dist/hadoop-common/core-default.xml

<configuration>
    <property>
        <name>fs.defaultFS</name>
        <value>file:///</value>
    </property>
    <property>
        <name>hadoop.tmp.dir</name>
        <value>/var/hadoop</value>
    </property>
</configuration>

3 配置 hdfs-site.xml
https://hadoop.apache.org/docs/r2.7.3/hadoop-project-dist/hadoop-hdfs/hdfs-default.xml

<configuration>
    <property>
        <name>dfs.replication</name>
        <value>2</value>
        <description> 文件复制份数 </description>
    </property>
    <property>
        <name>dfs.namenode.http-address</name>
        <value>192.168.4.10:50070</value>
    </property>
    <property>
        <name>dfs.namenode.secondary.http-address</name>
        <value>192.168.4.10:50090</value>
    </property>
</configuration>

常用配置选项
dfs.namenode.name.dir
dfs.datanode.data.dir
dfs.namenode.http-address
dfs.namenode.secondary.http-address
dfs.webhdfs.enabled
dfs.permissions.enabled

4 配置 mapred-site.xml
https://hadoop.apache.org/docs/r2.7.3/hadoop-mapreduce-client/hadoop-mapreduce-client-core/mapred-default.xml

<configuration>
    <property>
        <name>mapreduce.framework.name</name>
        <value>yarn</value>
    </property>
    <property>
        <name>mapreduce.jobtracker.http.address</name>
        <value>master:50030</value>
    </property>
    <property>
        <name>mapreduce.jobhistory.address</name>
        <value>master:10020</value>
    </property>
    <property>
        <name>mapreduce.jobhistory.webapp.address</name>
        <value>master:19888</value>
    </property>
</configuration>

常用配置选项
mapreduce.framework.name
mapreduce.jobtracker.http.address
mapreduce.jobhistory.address
mapreduce.jobhistory.webapp.address

5 配置 yarn-site.xml
https://hadoop.apache.org/docs/r2.7.3/hadoop-yarn/hadoop-yarn-common/yarn-default.xml

<configuration>

<!-- Site specific YARN configuration properties -->
    <property>
        <name>yarn.nodemanager.aux-services</name>
        <value>myhadoop</value>
    </property>

    <property>
        <name>yarn.resourcemanager.hostname</name>
        <value>master</value>
    </property>
</configuration>


常用配置选项
yarn.nodemanager.aux-services
yarn.nodemanager.aux-services.mapreduce.shuffle.class
yarn.resourcemanager.hostname


GFS是一个可扩展的分布式文件系统,
用于大型的,分布式的,对大量数据进行访问
的应用
GFS可以运行在廉价的普通硬件上,提供容错功能

Map/Reduce
Mapreduce是一种编程模型，是一种编程方法，抽象理论。
Map和Reduce其实是两种操作
Mapreduce是针对分布式并行计算的一套编程模型,
由 Map 和Reduce 组成,
Map是映射,
把指令分发到多个worker上,
Reduce 是规约,
把worker计算出的结果合并

BigTable存储结构化数据,是一个数据库
BigTable建立在GFS,Scheduler,Lock Service 和MapReduce之上
每个Table都是一个多维的稀疏图



Hadoop就是一个实现了Google云计算系统的开源系统，
包括并行计算模型Map/Reduce，
分布式文件系统HDFS，
以及分布式数据库Hbase，
同时Hadoop的相关项目也很丰富，
包括ZooKeeper，Pig，Chukwa，Hive，Hbase，Mahout，flume等.


Google三驾马车——GFS、MapReduce、Bigtable


/************ 仅仅是主机名介绍,对 本实验无影响*****
### 192.168.1.10    hadoop-nn01  对应  192.168.0.11  Va1

[root@room9pc01 ~]# unzip   /var/git/Hadoop.zip   -d   /var/ftp/
Archive:  /var/git/Hadoop.zip
  inflating: /var/ftp/hadoop/hadoop-2.7.6.tar.gz  
 extracting: /var/ftp/hadoop/kafka_2.10-0.10.2.1.tgz  
  inflating: /var/ftp/hadoop/zookeeper-3.4.10.tar.gz  

[root@room9pc01 ~]# ls  /var/ftp/
ansible  CentOS7-1708  elk  hadoop  pub  rhel7  share

[root@room9pc01 ~]# ls  /var/ftp/hadoop/
hadoop-2.7.6.tar.gz  kafka_2.10-0.10.2.1.tgz  zookeeper-3.4.10.tar.gz

[root@Va1 ~]# cat  /etc/hosts  #hadoop 对主机名强依赖,必须添加域名解析配置
127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4
::1         localhost localhost.localdomain localhost6 localhost6.localdomain6
192.168.0.11  Va1
192.168.1.11  Va1
192.168.0.12  Va2
192.168.1.12  Va2
192.168.0.13  Va3
192.168.1.13  Va3
192.168.0.14  Va4
192.168.1.14  Va4
192.168.0.15  Va5
192.168.1.15  Va5
192.168.0.16  Va6
192.168.1.16  Va6
192.168.0.17  Va7
192.168.1.17  Va7
192.168.0.18  Va8
192.168.1.18  Va8
192.168.0.19  Va9
192.168.1.19  Va9

-------------------------------------------------
[root@Va1 ~]# mkdir   /root/hadoop
[root@Va1 ~]# lftp  192.168.0.254
lftp 192.168.0.254:~> lcd  /root/hadoop/  #选择下载的家目录
lcd 成功, 本地目录=/root/hadoop
lftp 192.168.0.254:~> mget   hadoop/* #从 lftp服务器上一次下载多个文件
290212575 bytes transferred                                     
Total 3 files transferred
lftp 192.168.0.254:/> exit
[root@Va1 ~]# ls  /root/hadoop
hadoop/              hadoop-2.7.6.tar.gz  

[root@Va1 ~]# ls  /root/hadoop/
hadoop-2.7.6.tar.gz  kafka_2.10-0.10.2.1.tgz  zookeeper-3.4.10.tar.gz

[root@Va1 ~]# du  -sh  /root/hadoop/hadoop-2.7.6.tar.gz 
207M	/root/hadoop/hadoop-2.7.6.tar.gz

 ------------------------------------------- 安装java环境 -----------------------
[root@Va1 ~]# yum  -y  install  java-1.8.0-openjdk  java-1.8.0-openjdk-devel
........
已安装:
  java-1.8.0-openjdk-devel.x86_64 1:1.8.0.131-11.b12.el7                                

完毕！
[root@Va1 ~]# java  -version
openjdk version "1.8.0_131"
OpenJDK Runtime Environment (build 1.8.0_131-b12)
OpenJDK 64-Bit Server VM (build 25.131-b12, mixed mode)

JAVA JPS 命令详解

JPS 名称: jps - Java Virtual Machine Process Status Tool

jps 命令类似与 linux 的 ps 命令，
但是它只列出系统中所有的 Java 应用程序。 
通过 jps 命令可以方便地查看 Java 进程的启动类、传入参数和 Java 虚拟机参数等信息。

如果在 linux 中想查看 java 的进程，一般我们都需要 ps -ef | grep java 来获取进程 ID。

如果只想获取 Java 程序的进程，可以直接使用 jps 命令来直接查看。

jps可以列出jvm进程lvmid，主类类名，main函数参数, jvm参数，jar名称等信息。
参数说明
-q：只输出进程 ID
-m：输出传入 main 方法的参数
-l：输出完全的包名，应用主类名，jar的完全路径名
-v：输出jvm参数
-V：输出通过flag文件传递到JVM中的参数

没添加option的时候，默认列出VM标示符号和简单的class或jar名称.如下:
[root@Va1 ~]# jps  #
1121 Elasticsearch
6158 Jps

----------------------------------  安装hadoop  ---------------------------

[root@Va1 ~]# ls  /root/hadoop/
hadoop-2.7.6.tar.gz  kafka_2.10-0.10.2.1.tgz  zookeeper-3.4.10.tar.gz

[root@Va1 ~]# du  -sh  /root/hadoop/hadoop-2.7.6.tar.gz 
207M	/root/hadoop/hadoop-2.7.6.tar.gz

[root@Va1 ~]# tar  -xzvf  /root/hadoop/hadoop-2.7.6.tar.gz  -C   /usr/local/
..............
[root@Va1 ~]# mv   /usr/local/hadoop-2.7.6/    /usr/local/hadoop/
[root@Va1 ~]# ls  /usr/local/hadoop/
bin  etc  include  lib  libexec  LICENSE.txt  NOTICE.txt  README.txt  sbin  share

[root@Va1 ~]# free  -m
              total        used        free      shared  buff/cache   available
Mem:           1984        1622          76           1         285         168
Swap:          2047          10        2037

[root@Va1 ~]# systemctl  stop  elasticsearch  kibana && systemctl  disable  elasticsearch  kibana.service 

[root@Va1 ~]# free  -m
              total        used        free      shared  buff/cache   available
Mem:           1984         128        1572           1         283        1662
Swap:          2047          10        2037

[root@Va1 ~]# ls  /usr/local/hadoop/sbin/
distribute-exclude.sh    start-all.cmd        stop-balancer.sh
hadoop-daemon.sh         start-all.sh         stop-dfs.cmd
hadoop-daemons.sh        start-balancer.sh    stop-dfs.sh
hdfs-config.cmd          start-dfs.cmd        stop-secure-dns.sh
hdfs-config.sh           start-dfs.sh         stop-yarn.cmd
httpfs.sh                start-secure-dns.sh  stop-yarn.sh
kms.sh                   start-yarn.cmd       yarn-daemon.sh
mr-jobhistory-daemon.sh  start-yarn.sh        yarn-daemons.sh
refresh-namenodes.sh     stop-all.cmd
slaves.sh                stop-all.sh

[root@Va1 ~]# ls  /usr/local/hadoop/bin/
container-executor  hdfs      mapred.cmd               yarn
hadoop              hdfs.cmd  rcc                      yarn.cmd
hadoop.cmd          mapred    test-container-executor

[root@Va1 ~]# cd   /usr/local/hadoop/
[root@Va1 hadoop]# ./bin/hadoop   version   ## 查看 hadoop 版本
Error: JAVA_HOME is not set and could not be found.
            Java_Home未设置，找不到

[root@Va1 hadoop]# rpm  -ql  java-1.8.0-openjdk  |grep  jre  #查看 Java_Home 家目录路径
/usr/lib/jvm/java-1.8.0-openjdk-1.8.0.131-11.b12.el7.x86_64/jre/bin/policytool
/usr/lib/jvm/java-1.8.0-openjdk-1.8.0.131-11.b12.el7.x86_64/jre/lib/amd64/libawt_xawt.so
/usr/lib/jvm/java-1.8.0-openjdk-1.8.0.131-11.b12.el7.x86_64/jre/lib/amd64/libjawt.so
/usr/lib/jvm/java-1.8.0-openjdk-1.8.0.131-11.b12.el7.x86_64/jre/lib/amd64/libjsoundalsa.so
/usr/lib/jvm/java-1.8.0-openjdk-1.8.0.131-11.b12.el7.x86_64/jre/lib/amd64/libsplashscreen.so

[root@Va1 hadoop]# cd   /usr/local/hadoop/etc/hadoop/  #hadoop配置文件路径

[root@Va1 hadoop]# ll  hadoop-env.sh              #hadoop 运行环境变量的配置文件
-rw-r--r-- 1 20415 101 4224 4月  18 2018 hadoop-env.sh


[root@Va1 hadoop]# sed  -n  "/JAVA_HOME=/p;/HADOOP_CONF_DIR=/p"  hadoop-env.sh
export JAVA_HOME=${JAVA_HOME}                   #查看 Java_Home 家目录路径
export HADOOP_CONF_DIR=${HADOOP_CONF_DIR:-"/etc/hadoop"} #hadoop配置文件路径

--------------------------------- 修改配置文件的运行环境： ----------------------------------------

                                          # 设置 Java_Home 家目录路径
[root@Va1 hadoop]# sed  -i   "/JAVA_HOME=/s#\(=\).*#\1\"/usr/lib/jvm/java-1.8.0-openjdk-1.8.0.131-11.b12.el7.x86_64/jre\"#"   hadoop-env.sh

[root@Va1 hadoop]# ls   /usr/local/hadoop/etc/hadoop/  # 全是hadoop配置文件
........................
[root@Va1 hadoop]# ls   /usr/local/hadoop/etc/hadoop/  # 全是hadoop配置文件
capacity-scheduler.xml      httpfs-env.sh            mapred-env.sh
configuration.xsl           httpfs-log4j.properties  mapred-queues.xml.template
container-executor.cfg      httpfs-signature.secret  mapred-site.xml.template
core-site.xml               httpfs-site.xml          slaves
hadoop-env.cmd              kms-acls.xml             ssl-client.xml.example
hadoop-env.sh               kms-env.sh               ssl-server.xml.example
.......................   
                                                      # 设置 hadoop配置文件路径
[root@Va1 hadoop]# sed  -i  "/HADOOP_CONF_DIR=\${/s#\${HADOOP_CONF_DIR:-.*#\"/usr/local/hadoop/etc/hadoop/\"#"  hadoop-env.sh

                                                                          #查看 Java_Home 家目录路径 #hadoop配置文件路径
[root@Va1 hadoop]# egrep  -n  "JAVA_HOME=|HADOOP_CONF_DIR="  hadoop-env.sh

25:export JAVA_HOME="/usr/lib/jvm/java-1.8.0-openjdk-1.8.0.131-11.b12.el7.x86_64/jre"  #Java_Home 家目录路径
33:export HADOOP_CONF_DIR="/usr/local/hadoop/etc/hadoop/"   #hadoop配置文件路径

[root@Va1 hadoop]# grep  -Env "^(\s*#|$)"  hadoop-env.sh   #查看 Java_Home 家目录路径 #hadoop配置文件路径

                            #查看 Java_Home 家目录路径
25:export JAVA_HOME="/usr/lib/jvm/java-1.8.0-openjdk-1.8.0.131-11.b12.el7.x86_64/jre"

33:export HADOOP_CONF_DIR="/usr/local/hadoop/etc/hadoop/"  #hadoop配置文件路径

36:for f in $HADOOP_HOME/contrib/capacity-scheduler/*.jar; do
37:  if [ "$HADOOP_CLASSPATH" ]; then
38:    export HADOOP_CLASSPATH=$HADOOP_CLASSPATH:$f
39:  else
40:    export HADOOP_CLASSPATH=$f
41:  fi
42:done
.....................

[root@Va1 hadoop]# /usr/local/hadoop/bin/hadoop  version ## 查看 hadoop 版本
...................
[root@Va1 hadoop]# cd   /usr/local/hadoop/bin/
[root@Va1 bin]# ls
container-executor  hdfs      mapred.cmd               yarn
hadoop              hdfs.cmd  rcc                      yarn.cmd
hadoop.cmd          mapred    test-container-executor

[root@Va1 bin]# /usr/local/hadoop/bin/hadoop   version   ## 查看 hadoop 版本
Hadoop 2.7.6
Subversion https://shv@git-wip-us.apache.org/repos/asf/hadoop.git -r 085099c66cf28be31604560c376fa282e69282b8
Compiled by kshvachk on 2018-04-18T01:33Z
Compiled with protoc 2.5.0
From source with checksum 71e2695531cb3360ab74598755d036
This command was run using /usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.6.jar

[root@Va1 bin]# pwd
/usr/local/hadoop/bin
[root@Va1 bin]# cd  ../
[root@Va1 hadoop]# pwd
/usr/local/hadoop
[root@Va1 hadoop]# ls
bin  etc  include  lib  libexec  LICENSE.txt  NOTICE.txt  README.txt  sbin  share

[root@Va1 hadoop]# mkdir  newtest  # 创建数据源

[root@Va1 hadoop]# cp  *.txt  newtest/ # 创建数据源
[root@Va1 hadoop]# ls   newtest/
LICENSE.txt  NOTICE.txt  README.txt

license  英 [ˈlaɪsns]   美 [ˈlaɪsəns]  
n. 许可证，执照;特许
vt. 同意;发许可证
[root@Va1 hadoop]# ll  newtest/LICENSE.txt 
-rw-r--r-- 1 root root 86424 1月  24 19:47 newtest/LICENSE.txt

yarn    英 [jɑ:n]   美 [jɑ:rn]  
n. 纱，线;（尤指）毛线;故事;（旅行）轶事
vi. 讲故事

[root@Va1 hadoop]# ./bin/hadoop  --help  |grep  -A2  "jar <jar>"
  jar <jar>            run a jar file
                       note: please use "yarn jar" to launch
                             YARN applications, not this command.
           请使用“yarn jar”启动 yarn 应用程序，而不是此命令。

[root@Va1 hadoop]# ./bin/hadoop  jar  share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.6.jar
  
An example program must be given as the first argument.
必须给出一个示例程序作为第一个参数。
Valid program names are:
有效的程序名是：
  aggregatewordcount: An Aggregate based map/reduce program that counts the words in the input files.
...........................
  distbbp: A map/reduce program that uses a BBP-type formula to compute exact bits of Pi.
  grep: A map/reduce program that counts the matches of a regex in the input.
  join: A job that effects a join over sorted, equally partitioned datasets
................
  sort: A map/reduce program that sorts the data written by the random writer.
..................
  wordcount: A map/reduce program that counts the words in the input files.
  wordmean: A map/reduce program that counts the average length of the words in the input files.
...............
[root@Va1 hadoop]# ./bin/hadoop  jar  share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.6.jar   |grep  wordcount

聚合字计数 : 一种基于聚合的映射/减少程序，对输入文件中的字进行计数。
  aggregatewordcount: An Aggregate based map/reduce program that counts the words in the input files.
  wordcount: A map/reduce program that counts the words in the input files.
                对输入文件中的字进行计数的映射/减少程序。


# 可执行文件/usr/local/hadoop/bin/hadoop  jar（使用hadoop运行jar包）   运行的jar文件(.jar包)   (要使用的类名)程序名wordcount  (参数)输入的文件路径(已有的)   (参数)输出的文件路径(可以是还不存在的文件夹)

[root@Va1 hadoop]# ./bin/hadoop  jar  share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.6.jar   wordcount
Usage: wordcount <in> [<in>...] <out>

# 可执行文件/usr/local/hadoop/bin/hadoop  jar  运行的jar文件(.jar包)   (参数)程序名wordcount  输入的文件路径(已有的)   输出的文件路径(可以是还不存在的文件夹)
  ----------------------------统计分析1  单词出现的频率 ----------------------------

[root@Va1 hadoop]# ./bin/hadoop  jar  share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.6.jar   wordcount  newtest  new2  # 统计出现频率(对输入文件中的字进行计数)
...................
		WRONG_REDUCE=0
	File Input Format Counters 
		Bytes Read=102768
	File Output Format Counters 
		Bytes Written=30538
[root@Va1 hadoop]# ll  new2/
总用量 32
-rw-r--r-- 1 root root 30290 1月  24 19:57 part-r-00000
-rw-r--r-- 1 root root     0 1月  24 19:57 _SUCCESS

[root@Va1 hadoop]# head  -2  new2/part-r-00000
""AS	2
"AS	17
[root@Va1 hadoop]# tail   -3  new2/part-r-00000
“You”	2
“commercial	3
“control”	1

[root@Va1 hadoop]# grep  701  new2/part-r-00000 ##查看结果中 统计出现频率最大的字词
252.227-7014(a)(1))	1
the	701
 252.227-7014(a)(1))	1


[root@Va1 hadoop]# mkdir   olddir/   # 创建数据源
[root@Va1 hadoop]# cp  *.txt   olddir/

                    -------------------------------------- 统计分析1  单词出现的频率 ------------------------------------

[root@Va1 hadoop]# /usr/local/hadoop/bin/hadoop  jar    share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.6.jar   wordcount   olddir/   newdir

# 可执行文件/usr/local/hadoop/bin/hadoop  jar（使用hadoop运行jar包）   运行的jar文件(.jar包)   (要使用的类名)程序名wordcount  (参数)输入的文件路径(已有的)   (参数)输出的文件路径(可以是还不存在的文件夹)

19/01/24 20:51:59 INFO Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id
19/01/24 20:51:59 INFO jvm.JvmMetrics: Initializing JVM Metrics with processName=JobTracker, sessionId=
19/01/24 20:51:59 INFO input.FileInputFormat: Total input paths to process : 3
19/01/24 20:51:59 INFO mapreduce.JobSubmitter: number of splits:3
....................
	File System Counters
		FILE: Number of bytes read=1665799
		FILE: Number of bytes written=2587463
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
	Map-Reduce Framework
		Map input records=2062
.................
	Shuffle Errors
        ...............
		WRONG_REDUCE=0
	File Input Format Counters 
		Bytes Read=102768
	File Output Format Counters 
		Bytes Written=30538
[root@Va1 hadoop]#  ll  newdir/
总用量 32
-rw-r--r-- 1 root root 30290 1月  24 20:51 part-r-00000
-rw-r--r-- 1 root root     0 1月  24 20:51 _SUCCESS

非打印字符
非打印字符也可以是正则表达式的组成部分。下表列出了表示非打印字符的转义序列：
\s	匹配任何空白字符，包括空格、制表符、换页符等等。等价于 [ \f\n\r\t\v]。注意 Unicode 正则表达式会匹配全角空格符。
\S	匹配任何非空白字符。等价于 [^ \f\n\r\t\v]。
\t	匹配一个制表符。等价于 \x09 和 \cI。
\v	匹配一个垂直制表符。等价于 \x0b 和 \cK。
\f	匹配一个换页符。等价于 \x0c 和 \cL。
\n	匹配一个换行符。等价于 \x0a 和 \cJ。
\r	匹配一个回车符。等价于 \x0d 和 \cM。
http://www.runoob.com/regexp/regexp-syntax.html

[root@Va1 hadoop]# egrep  -n  "\s([0-9]){3,}"   newdir/part-r-00000 
124:*	174
768:OF	169
770:OR	170
930:THE	116
1028:You	125
1037:a	157
1105:and	285
1110:any	129
1197:by	134
1657:in	174
1710:is	100
1920:of	448
1941:or	277
2265:that	119
2267:the	701
2283:this	183
2289:to	253
2369:with	105
[root@Va1 hadoop]# ls  newdir/
part-r-00000  _SUCCESS
[root@Va1 hadoop]# ls  olddir/
LICENSE.txt  NOTICE.txt  README.txt

[root@Va1 hadoop]# cat  newdir/_SUCCESS 
[root@Va1 hadoop]# ll  newdir/_SUCCESS
-rw-r--r-- 1 root root 0 1月  24 20:51 newdir/_SUCCESS

# 可执行文件/usr/local/hadoop/bin/hadoop  jar（使用hadoop运行jar包）   运行的jar文件(.jar包)   (要使用的类名)程序名wordcount  (参数)输入的文件路径(已有的)   (参数)输出的文件路径(可以是还不存在的文件夹)

[root@Va1 hadoop]# /usr/local/hadoop/bin/hadoop  jar    share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.6.jar   wordcount   olddir/   newdir  
         # 报错提示文件已经存在/usr/local/hadoop/newdir already exists

19/01/24 21:08:39 INFO Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id
19/01/24 21:08:39 INFO jvm.JvmMetrics: Initializing JVM Metrics with processName=JobTracker, sessionId=
org.apache.hadoop.mapred.FileAlreadyExistsException: Output directory file:/usr/local/hadoop/newdir already exists
........................

[root@Va1 hadoop]# 统计分析2  某一个关键词出现的频率，例如 dfs 这个词前面字母是 h 的出现的频率

# 可执行文件/usr/local/hadoop/bin/hadoop  jar（使用hadoop运行jar包）   运行的jar文件(.jar包)   (要使用的类名)程序名wordcount  (参数)输入的文件路径(已有的)   (参数)输出的文件路径(可以是还不存在的文件夹)

[root@Va1 hadoop]# /usr/local/hadoop/bin/hadoop  jar    share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.6.jar   grep   olddir/  newdir2   '(?<=h)dfs'

19/01/24 21:25:11 INFO Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id
19/01/24 21:25:11 INFO jvm.JvmMetrics: Initializing JVM Metrics with processName=JobTracker, sessionId=
.........................
	File Input Format Counters 
		Bytes Read=118
	File Output Format Counters 
		Bytes Written=19
[root@Va1 hadoop]# ls   
bin  include  libexec      newdir   NOTICE.txt  README.txt  share
etc  lib      LICENSE.txt  newdir2  olddir      sbin
[root@Va1 hadoop]# ls   newdir2/
part-r-00000  _SUCCESS

[root@Va1 hadoop]# cat  newdir2/part-r-00000
10	dfs
------------- 统计分析  某一个关键词出现的频率，例如 he 这个词前面字母是 t 的出现的频率 ----------

[root@Va1 hadoop]# /usr/local/hadoop/bin/hadoop  jar    share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.6.jar   grep   olddir/  newdir3  '(?<=t)he'
..............
	      WRONG_REDUCE=0
	File Input Format Counters 
		Bytes Read=117
	File Output Format Counters 
		Bytes Written=19
[root@Va1 hadoop]# ls
bin  include  libexec      newdir   newdir3     olddir      sbin
etc  lib      LICENSE.txt  newdir2  NOTICE.txt  README.txt  share
[root@Va1 hadoop]# ls   newdir3/
part-r-00000  _SUCCESS
[root@Va1 hadoop]# cat  newdir3/part-r-00000 
891	he
[root@Va1 ~]# free  -m
              total        used        free      shared  buff/cache   available
Mem:           1984         107        1736           8         140        1717
Swap:          2047           0        2047
[root@Va1 ~]# echo  "scale=2;1024/128" |bc
8.00
[root@Va1 ~]# echo  "scale=2;1024%128" |bc
0
[root@Va1 ~]# echo  "scale=2;128*8" |bc
1024
 [root@Va1 ~]# egrep  -nv  "^(\s*#|$)"  /usr/local/hadoop/etc/hadoop/hadoop-env.sh 
25:export JAVA_HOME="/usr/lib/jvm/java-1.8.0-openjdk-1.8.0.131-11.b12.el7.x86_64/jre"
33:export HADOOP_CONF_DIR="/usr/local/hadoop/etc/hadoop/"
36:for f in $HADOOP_HOME/contrib/capacity-scheduler/*.jar; do
37:  if [ "$HADOOP_CLASSPATH" ]; then
38:    export HADOOP_CLASSPATH=$HADOOP_CLASSPATH:$f
39:  else
40:    export HADOOP_CLASSPATH=$f
41:  fi
42:done
49:export HADOOP_OPTS="$HADOOP_OPTS -Djava.net.preferIPv4Stack=true"
52:export HADOOP_NAMENODE_OPTS="-Dhadoop.security.logger=${HADOOP_SECURITY_LOGGER:-INFO,RFAS} -Dhdfs.audit.logger=${HDFS_AUDIT_LOGGER:-INFO,NullAppender} $HADOOP_NAMENODE_OPTS"
53:export HADOOP_DATANODE_OPTS="-Dhadoop.security.logger=ERROR,RFAS $HADOOP_DATANODE_OPTS"
55:export HADOOP_SECONDARYNAMENODE_OPTS="-Dhadoop.security.logger=${HADOOP_SECURITY_LOGGER:-INFO,RFAS} -Dhdfs.audit.logger=${HDFS_AUDIT_LOGGER:-INFO,NullAppender} $HADOOP_SECONDARYNAMENODE_OPTS"
57:export HADOOP_NFS3_OPTS="$HADOOP_NFS3_OPTS"
58:export HADOOP_PORTMAP_OPTS="-Xmx512m $HADOOP_PORTMAP_OPTS"
61:export HADOOP_CLIENT_OPTS="-Xmx512m $HADOOP_CLIENT_OPTS"
69:export HADOOP_SECURE_DN_USER=${HADOOP_SECURE_DN_USER}
75:export HADOOP_SECURE_DN_LOG_DIR=${HADOOP_LOG_DIR}/${HADOOP_HDFS_USER}
94:export HADOOP_PID_DIR=${HADOOP_PID_DIR}
95:export HADOOP_SECURE_DN_PID_DIR=${HADOOP_PID_DIR}
98:export HADOOP_IDENT_STRING=$USER

[root@Va1 ~]# ls  /usr/local/hadoop/
bin  include  libexec      newdir   newdir3     olddir      sbin
etc  lib      LICENSE.txt  newdir2  NOTICE.txt  README.txt  share

[root@Va1 ~]# cd   /usr/local/hadoop/bin/ ;ls
container-executor  hdfs      mapred.cmd               yarn
hadoop              hdfs.cmd  rcc                      yarn.cmd
hadoop.cmd          mapred    test-container-executor

[root@Va1 bin]# head  -4 /usr/local/hadoop/bin/hadoop
#!/usr/bin/env bash

# Licensed to the Apache Software Foundation (ASF) under one or more
# contributor license agreements.  See the NOTICE file distributed with

[root@Va1 bin]# tail  -4 /usr/local/hadoop/bin/hadoop

    exec "$JAVA" $JAVA_HEAP_MAX $HADOOP_OPTS $CLASS "$@"
    ;;

esac
[root@Va1 bin]# file  /usr/bin/env
/usr/bin/env: ELF 64-bit LSB executable, x86-64, version 1 (SYSV), dynamically linked 
(uses shared libs), for GNU/Linux 2.6.32, BuildID[sha1]=4279a25ddbac2a7480923cd05d70e33a73dce721, stripped
 ELF 电子测位器，电子定位器(Electronic Location Finder)
一种为Linux系统所采用的通用文件格式，支持动态连接

[root@Va1 bin]# file   /usr/local/hadoop/bin/hadoop
/usr/local/hadoop/bin/hadoop: a /usr/bin/env bash script, ASCII text executable

 ###  /usr/local/hadoop/bin/hadoop  version ## 查看 hadoop 版本

[root@Va1 bin]# cd    /usr/local/hadoop/etc/hadoop/
[root@Va1 hadoop]# ls
capacity-scheduler.xml      kms-env.sh
configuration.xsl           kms-log4j.properties
container-executor.cfg      kms-site.xml
core-site.xml               log4j.properties
hadoop-env.cmd              mapred-env.cmd
hadoop-env.sh               mapred-env.sh
hadoop-metrics2.properties  mapred-queues.xml.template
hadoop-metrics.properties   mapred-site.xml.template
hadoop-policy.xml           slaves
hdfs-site.xml               ssl-client.xml.example
httpfs-env.sh               ssl-server.xml.example
httpfs-log4j.properties     yarn-env.cmd
httpfs-signature.secret     yarn-env.sh
httpfs-site.xml             yarn-site.xml
kms-acls.xml

Hadoop Distributed File  System(HDFS)：
Hadoop分布式文件系统，
提供 高吞吐量 应用程序 数据访问，并具有高容错性。

 Hadoop常用组件以及核心组件有哪些

HDFS：Hadoop分布式文件系统（核心组件）
MapReduce：分布式计算框架（核心组件）
Yarn：集群资源管理系统（核心组件）

Zookeeper：分布式协作服务
Hbase：分布式列存数据库
Hive：基于Hadoop的数据仓库
Sqoop：数据同步工具
Pig：基于Hadoop的数据流系统
Mahout：数据挖掘算法库
Flume：日志收集工具

Yarn，英文全名是 Yet Another Resource Negotiator，
是由雅虎开发的第二代集群资源调度器

[root@Va1 hadoop]# pwd
/usr/local/hadoop/etc/hadoop
[root@Va1 hadoop]# cp   mapred-site.xml.template  mapred-site.xml
[root@Va1 hadoop]# ll  mapred-site.xml
-rw-r--r-- 1 root root 758 1月  25 15:24 mapred-site.xml
[root@Va1 hadoop]# ll  core-site.xml   hadoop-env.sh   hdfs-site.xml   mapred-site.xml   slaves   yarn-site.xml 
-rw-r--r-- 1 20415  101  774 4月  18 2018 core-site.xml # 全局配置文件
-rw-r--r-- 1 20415  101 4275 1月  24 19:06 hadoop-env.sh
-rw-r--r-- 1 20415  101  775 4月  18 2018 hdfs-site.xml  # HDFS：Hadoop分布式文件系统（核心组件）
-rw-r--r-- 1 root  root  758 1月  25 15:24 mapred-site.xml # MapReduce：分布式计算框架（核心组件）
-rw-r--r-- 1 20415  101   10 4月  18 2018 slaves  # 节点配置文件(主机名)
-rw-r--r-- 1 20415  101  690 4月  18 2018 yarn-site.xml  # Yarn：集群资源管理系统（核心组件）

                          #查看 Java_Home 家目录路径 #hadoop配置文件路径
[root@Va1 hadoop]# egrep  -n  "JAVA_HOME=|HADOOP_CONF_DIR="  hadoop-env.sh

25:export JAVA_HOME="/usr/lib/jvm/java-1.8.0-openjdk-1.8.0.131-11.b12.el7.x86_64/jre" 
         #Java_Home 家目录路径
33:export HADOOP_CONF_DIR="/usr/local/hadoop/etc/hadoop/"   #hadoop配置文件路径










